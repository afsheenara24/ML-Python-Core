# ML-Python-Core - Day 1

This repository contains my Day 1 work for building strong Machine Learning foundations using Python.

## Day 1 – Python for Machine Learning

### Topics covered:
- Python basics for ML (functions, list comprehension, lambda)
- NumPy fundamentals (ndarray, vectorization, broadcasting)
- Performance comparison: loops vs vectorized operations
- Pandas basics (DataFrame, data inspection, cleaning)
- Simple feature engineering

### Files

- day01_numpy_basics.ipynb  
  Covers NumPy arrays, vectorized operations, broadcasting, and matrix math.

- day01_pandas_basics.ipynb  
  Covers data loading, inspection, handling missing values, and feature creation.

## Day 2 – Advanced NumPy & Linear Regression

### Topics Covered
- Advanced NumPy operations and vectorization
- Exploratory Data Analysis (EDA)
- Data visualization using Matplotlib
- Train-test split
- Linear Regression model training
- Model evaluation using MSE and R²
- Underfitting analysis

### Key Learnings
- Visualization helps in selecting appropriate models
- Linear regression can underfit when features are insufficient
- Evaluation metrics confirm model limitations

##  Day 3 – Polynomial Regression & Model Evaluation

### Topics Covered
- Polynomial feature transformation
- Linear vs Polynomial Regression
- Train-test split for model validation
- Model performance comparison
- Evaluation metrics: R², MAE, RMSE
- Bias–variance tradeoff
- Overfitting and underfitting analysis

### Key Learnings
- Polynomial regression captures non-linear relationships better than linear models
- Higher model complexity can lead to overfitting if not controlled
- Train-test split helps detect generalization issues
- R² alone is insufficient to evaluate model performance.
- Error-based metrics like MAE and RMSE provide real-world insight into prediction accuracy

## Day 4 – Multiple Linear Regression

### Topics Covered
- Introduction to Multiple Linear Regression (MLR)
- Mathematical formulation of MLR
- Assumptions of linear regression
- Feature scaling and normalization
- Multiple Linear Regression from scratch using NumPy
- Gradient Descent optimization
- Train–test split and data leakage prevention
- Multiple Linear Regression using scikit-learn
- Model evaluation using MSE and R²
- Residual analysis
- Multicollinearity detection using Variance Inflation Factor (VIF)
- Interpretation of regression coefficients

### Key Learnings
- Multiple features improve model expressiveness compared to simple linear regression
- Feature scaling is essential for stable and faster convergence
- Train–test split ensures reliable performance evaluation
- Residual analysis helps validate model assumptions
- Multicollinearity negatively impacts coefficient stability and interpretability
- Regression coefficients provide actionable insights into feature importance

## Tools Used

- Python 3.10
- NumPy
- Pandas
- Jupyter Notebook
- Conda environment

## How to Run

1. Activate conda environment
2. Open Jupyter Notebook or JupyterLab
3. Run the notebooks cell by cell

## Goal

To build strong fundamentals in Python and data handling as a foundation for Machine Learning and AI projects.

